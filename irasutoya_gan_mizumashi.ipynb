{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "irasutoya_gan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZswLTOO-u0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxPX4j7e-4Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE4SuFtJCqr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6WKa490_sbd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "f4d1ae36-d4a0-4df0-c262-04fc2f6e4661"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0X3E5vs_tUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c39f635d-3d68-4da4-a4f8-be83a4475961"
      },
      "source": [
        "!ls 'drive/My Drive'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   datasets   generate_imgs   saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ830H2__-LL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1129de35-b715-419b-d847-61ff9d965591"
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import keras as K\n",
        "from keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091ZYhm-ACUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator(z_dim):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # 8x8x256\n",
        "    model.add(Dense(8 * 8 * 256, input_dim=z_dim))\n",
        "    model.add(Reshape((8, 8, 256)))\n",
        "\n",
        "    # 8x8x256 => 8x8x128\n",
        "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    # 8x8x128 => 16x16x64\n",
        "    model.add(Conv2DTranspose(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    # 16x16x64 => 32x32x32\n",
        "    model.add(Conv2DTranspose(32, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    # 32x32x32 => 64x64x16\n",
        "    model.add(Conv2DTranspose(16, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    # 64x64x16 => 64x64x3\n",
        "    model.add(Conv2DTranspose(3, kernel_size=3, strides=1, padding='same'))\n",
        "\n",
        "    # tanh関数を適用して出力\n",
        "    model.add(Activation('tanh'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hLDc6GkAF0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(img_shape):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # 64x64x3 => 32x32x32\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # 32x32x32 => 16x16x64\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "    # 16x16x64 => 8x8x128\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCEyvghpAJYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gan(generator, discriminator):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxQs5FdRAMOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(iterations, batch_size, sample_interval):\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    iteration_checkpoints = []\n",
        "\n",
        "    dataset = np.load('drive/My Drive/datasets/irasutoya_64.npz')\n",
        "\n",
        "    x_train = np.array(dataset['X_train'])\n",
        "\n",
        "     # 本物の画像のラベルは全て1にする\n",
        "    real = np.ones((batch_size, 1))\n",
        "\n",
        "    # 偽物の画像のラベルは全て0にする\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        #-------------\n",
        "        # 識別器の学習\n",
        "        #-------------\n",
        "\n",
        "        # 本物の画像集合からランダムにバッチを生成する\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        imgs = x_train[idx]\n",
        "\n",
        "        # 偽物の画像からなるバッチを生成する\n",
        "        z = np.random.normal(-1, 1, (batch_size, z_dim))\n",
        "        gen_imgs = generator.predict(z)\n",
        "\n",
        "        # 識別器の学習\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, real)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss, accuracy = 0.5 * np.add(d_loss_real , d_loss_fake)\n",
        "\n",
        "        #-------------\n",
        "        # 生成器の学習\n",
        "        #-------------\n",
        "        for i in range(2):\n",
        "            # ノイズベクトルを生成\n",
        "            z = np.random.normal(-1, 1, (batch_size, z_dim))    \n",
        "            # 生成器の学習\n",
        "            g_loss = gan.train_on_batch(z, real)\n",
        "\n",
        "        print('\\rNo, %d' %(iteration+1), end='')\n",
        "\n",
        "        if (iteration + 1) % sample_interval == 0:\n",
        "\n",
        "            # あとで可視化するために損失と精度を保存しておく\n",
        "            losses.append([d_loss, g_loss])\n",
        "            accuracies.append(100.0 * accuracy)\n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "            # 学習結果の出力\n",
        "            print('[D loss: %f, acc.: %.2f%%] [G loss: %f]' %(d_loss, 100.0*accuracy, g_loss))\n",
        "\n",
        "            sample_images(generator, iteration)\n",
        "\n",
        "            generator.save(\"drive/My Drive/saved_model/irasutoya/1/%d_Noda.h5\" % (iteration+1))\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lFr6J0FASRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_images(generator, iteration, image_grid_rows=4, image_grid_columns=4):\n",
        "\n",
        "    # ノイズベクトルを生成する\n",
        "    z = np.random.normal(-1, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # ノイズベクトルから画像を生成する\n",
        "    gen_imgs = generator.predict(z)\n",
        "\n",
        "    # 出力の画素値を[0, 1]の範囲にスケーリングする\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "\n",
        "    # 画像からなるグリッドを生成する\n",
        "    fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(4, 4), sharey=True, sharex=True)\n",
        "    count = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # 並べた画像を出力\n",
        "            axs[i, j].imshow(gen_imgs[count, :, :, :])\n",
        "            axs[i, j].axis('off')\n",
        "            count += 1\n",
        "    \n",
        "    fig.savefig(\"drive/My Drive/generate_imgs/irasutoya/1/%d.png\" % (iteration+1))\n",
        "    plt.close()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U_X4mgSAVws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "aa7767b0-7b08-4333-84f0-aabdc98ee2dd"
      },
      "source": [
        "img_rows = 64\n",
        "img_cols = 64\n",
        "channels = 3\n",
        "\n",
        "iterations = 1000000\n",
        "batch_size = 64\n",
        "sample_interval = 1000\n",
        "\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "z_dim = 100\n",
        "\n",
        "# 識別器\n",
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# 生成器\n",
        "generator = build_generator(z_dim)\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy')\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\rNo, 1\rNo, 2"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No, 1000[D loss: 0.892081, acc.: 30.47%] [G loss: 0.816203]\n",
            "No, 2000[D loss: 0.835465, acc.: 27.34%] [G loss: 0.692886]\n",
            "No, 3000[D loss: 0.544286, acc.: 75.00%] [G loss: 0.311851]\n",
            "No, 4000[D loss: 0.248101, acc.: 96.88%] [G loss: 0.222163]\n",
            "No, 5000[D loss: 0.644871, acc.: 63.28%] [G loss: 0.727962]\n",
            "No, 6000[D loss: 0.362400, acc.: 85.16%] [G loss: 1.285770]\n",
            "No, 7000[D loss: 0.124064, acc.: 97.66%] [G loss: 0.226725]\n",
            "No, 8000[D loss: 2.680827, acc.: 11.72%] [G loss: 3.918017]\n",
            "No, 9000[D loss: 0.297635, acc.: 89.84%] [G loss: 0.964192]\n",
            "No, 10000[D loss: 0.105564, acc.: 98.44%] [G loss: 0.673275]\n",
            "No, 11000[D loss: 0.036149, acc.: 99.22%] [G loss: 0.364819]\n",
            "No, 11128"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}